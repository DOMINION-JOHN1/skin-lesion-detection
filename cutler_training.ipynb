{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DOMINION-JOHN1/skin-lesion-detection/blob/main/cutler_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6KuEcMCvjjz",
        "outputId": "50b4415a-f625-4128-b890-51fd7d1bb8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/MyDrive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "drive_mount_point = '/content/drive/MyDrive/'\n",
        "\n",
        "# Check if the directory exists, and create it if it doesn't\n",
        "if not os.path.exists(drive_mount_point):\n",
        "    os.makedirs(drive_mount_point)\n",
        "\n",
        "# Now, mount Google Drive to the specified directory\n",
        "from google.colab import drive\n",
        "drive.mount(drive_mount_point, force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD9v_HJh378d",
        "outputId": "31424aef-24de-4961-e278-2f9acffa4d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CutLER'...\n",
            "remote: Enumerating objects: 581, done.\u001b[K\n",
            "remote: Counting objects: 100% (424/424), done.\u001b[K\n",
            "remote: Compressing objects: 100% (333/333), done.\u001b[K\n",
            "remote: Total 581 (delta 115), reused 366 (delta 89), pack-reused 157\u001b[K\n",
            "Receiving objects: 100% (581/581), 44.84 MiB | 38.82 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n",
            "Submodule 'third_party/TokenCut' (https://github.com/YangtaoWANG95/TokenCut.git) registered for path 'third_party/TokenCut'\n",
            "Cloning into '/content/CutLER/cutler/demo/CutLER/third_party/TokenCut'...\n",
            "remote: Enumerating objects: 212, done.        \n",
            "remote: Counting objects: 100% (62/62), done.        \n",
            "remote: Compressing objects: 100% (38/38), done.        \n",
            "remote: Total 212 (delta 42), reused 32 (delta 23), pack-reused 150        \n",
            "Receiving objects: 100% (212/212), 7.29 MiB | 12.28 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n",
            "Submodule path 'third_party/TokenCut': checked out '5eba77f56f0fc92ffc79589d0cbb119aab5f9b2d'\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-tlaxfq1s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-tlaxfq1s\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit afe9eb920646102f7e6bf0cd2115841cea2aca13\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.15.2)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.10.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "--2024-04-02 09:10:26--  http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_cascade_final.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.34, 3.162.163.19, 3.162.163.11, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.34|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 574672112 (548M) [binary/octet-stream]\n",
            "Saving to: ‘cutler_cascade_final.pth’\n",
            "\n",
            "cutler_cascade_fina 100%[===================>] 548.05M   262MB/s    in 2.1s    \n",
            "\n",
            "2024-04-02 09:10:28 (262 MB/s) - ‘cutler_cascade_final.pth’ saved [574672112/574672112]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/facebookresearch/CutLER\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!wget http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_cascade_final.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQLAP8RMhtN_",
        "outputId": "393f81a2-f9d3-4eaf-b849-885a69cbc8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CutLER/cutler/demo/CutLER\n"
          ]
        }
      ],
      "source": [
        "%cd CutLER/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fjoW6X9h2FZn",
        "outputId": "7218cdae-d52e-4c2f-c605-0b2a2c0d2fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-t07aiyw0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-t07aiyw0\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (9.4.0)\n",
            "Collecting git+https://github.com/mcordts/cityscapesScripts.git\n",
            "  Cloning https://github.com/mcordts/cityscapesScripts.git to /tmp/pip-req-build-9rk7kx86\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mcordts/cityscapesScripts.git /tmp/pip-req-build-9rk7kx86\n",
            "  Resolved https://github.com/mcordts/cityscapesScripts.git to commit a7ac7b4062d1a80ed5e22d2ea2179c886801c77d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (3.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (1.4.4)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (0.9.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (15.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.10/dist-packages (from cityscapesScripts==2.2.2) (3.7.4.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->cityscapesScripts==2.2.2) (10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesScripts==2.2.2) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesScripts==2.2.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesScripts==2.2.2) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesScripts==2.2.2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesScripts==2.2.2) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesScripts==2.2.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesScripts==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->cityscapesScripts==2.2.2) (1.16.0)\n",
            "Collecting submitit (from -r requirements.txt (line 1))\n",
            "  Using cached submitit-1.5.1-py3-none-any.whl (74 kB)\n",
            "Collecting faiss-gpu==1.7.2 (from -r requirements.txt (line 4))\n",
            "  Using cached faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "Collecting opencv-python==4.6.0.66 (from -r requirements.txt (line 5))\n",
            "  Using cached opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "Collecting scikit-image==0.19.2 (from -r requirements.txt (line 6))\n",
            "  Using cached scikit_image-0.19.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "Collecting scikit-learn==1.1.1 (from -r requirements.txt (line 7))\n",
            "  Using cached scikit_learn-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.4 MB)\n",
            "Collecting shapely==1.8.2 (from -r requirements.txt (line 8))\n",
            "  Using cached Shapely-1.8.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "Collecting timm==0.5.4 (from -r requirements.txt (line 9))\n",
            "  Using cached timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "Collecting pyyaml==6.0 (from -r requirements.txt (line 10))\n",
            "  Using cached PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "Collecting colored (from -r requirements.txt (line 11))\n",
            "  Using cached colored-2.2.4-py3-none-any.whl (16 kB)\n",
            "Collecting fvcore==0.1.5.post20220512 (from -r requirements.txt (line 12))\n",
            "  Using cached fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gdown==4.5.4 (from -r requirements.txt (line 13))\n",
            "  Using cached gdown-4.5.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.6.0.66->-r requirements.txt (line 5)) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r requirements.txt (line 6)) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r requirements.txt (line 6)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r requirements.txt (line 6)) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r requirements.txt (line 6)) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r requirements.txt (line 6)) (24.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.1->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.1->-r requirements.txt (line 7)) (3.4.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4->-r requirements.txt (line 9)) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4->-r requirements.txt (line 9)) (0.17.1+cu121)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20220512->-r requirements.txt (line 12)) (0.1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20220512->-r requirements.txt (line 12)) (4.66.2)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20220512->-r requirements.txt (line 12)) (2.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20220512->-r requirements.txt (line 12)) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20220512->-r requirements.txt (line 12)) (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.5.4->-r requirements.txt (line 13)) (3.13.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.5.4->-r requirements.txt (line 13)) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.5.4->-r requirements.txt (line 13)) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.5.4->-r requirements.txt (line 13)) (4.12.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 1)) (4.10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore==0.1.5.post20220512->-r requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9)) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 9)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->timm==0.5.4->-r requirements.txt (line 9))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.5.4->-r requirements.txt (line 13)) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.5.4->-r requirements.txt (line 13)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.5.4->-r requirements.txt (line 13)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.5.4->-r requirements.txt (line 13)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.5.4->-r requirements.txt (line 13)) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.5.4->-r requirements.txt (line 13)) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.5.4->-r requirements.txt (line 9)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.5.4->-r requirements.txt (line 9)) (1.3.0)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61259 sha256=f7e8f4de5681e183c47a654062dcc2f69bd9b23326ee0b03918385966b6e0193\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/8f/c9/1da09aeb3f6ffbc1835fea895c84268a23124e085759da5031\n",
            "Successfully built fvcore\n",
            "Installing collected packages: faiss-gpu, submitit, shapely, pyyaml, opencv-python, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colored, scikit-learn, scikit-image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, gdown, fvcore, timm\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.3\n",
            "    Uninstalling shapely-2.0.3:\n",
            "      Successfully uninstalled shapely-2.0.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.19.3\n",
            "    Uninstalling scikit-image-0.19.3:\n",
            "      Successfully uninstalled scikit-image-0.19.3\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "  Attempting uninstall: fvcore\n",
            "    Found existing installation: fvcore 0.1.5.post20221221\n",
            "    Uninstalling fvcore-0.1.5.post20221221:\n",
            "      Successfully uninstalled fvcore-0.1.5.post20221221\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.0.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colored-2.2.4 faiss-gpu-1.7.2 fvcore-0.1.5.post20220512 gdown-4.5.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 opencv-python-4.6.0.66 pyyaml-6.0 scikit-image-0.19.2 scikit-learn-1.1.1 shapely-1.8.2 submitit-1.5.1 timm-0.5.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "fvcore",
                  "yaml"
                ]
              },
              "id": "fe615ca7cb8c4909892e3da103032658"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "!pip install git+https://github.com/mcordts/cityscapesScripts.git\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jcQIsHG223_",
        "outputId": "d3ef2f34-24bc-424e-d35c-fdfb8f553e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CutLER/cutler/demo/CutLER/cutler/demo\n"
          ]
        }
      ],
      "source": [
        "%cd cutler/demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDWEO-puvNXd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from detectron2.utils.logger import setup_logger\n",
        "import sys\n",
        "sys.path.append('./')\n",
        "sys.path.append('../')\n",
        "from config import add_cutler_config\n",
        "from predictor import VisualizationDemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cka9Kd3pvtle"
      },
      "outputs": [],
      "source": [
        "# ----------------------- Dataset registration -----------------------\n",
        "def register_dataset(name, json_path, img_dir):\n",
        "    if name not in DatasetCatalog.list():\n",
        "        register_coco_instances(name, {}, json_path, img_dir)\n",
        "    else:\n",
        "        print(f\"Dataset {name} is already registered.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y0jTK0zvu8R"
      },
      "outputs": [],
      "source": [
        "# Debug: List all registered datasets to ensure correct registration\n",
        "def list_registered_datasets():\n",
        "    print(\"Registered datasets:\")\n",
        "    for dataset_name in DatasetCatalog.list():\n",
        "        print(f\"- {dataset_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwZO5ZQfvze8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from detectron2.structures import BoxMode\n",
        "def yolo_to_coco(image_dir, label_dir, output_path, dataset_type):\n",
        "    images = []\n",
        "    annotations = []\n",
        "    # Define category mappings\n",
        "    categories = [{'id': 0, 'name': 'unlabeled'}]  # Replace with your actual category names\n",
        "    annotation_id = 1\n",
        "    # Get all image files\n",
        "    image_files = glob(os.path.join(image_dir, '*.png'))\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        # Generate a random UUID for the image ID\n",
        "        image_id = uuid.uuid4()\n",
        "        image_info = {\n",
        "            'file_name': os.path.basename(image_file),\n",
        "            'height': 640,\n",
        "            'width': 640,\n",
        "            'id': str(image_id)  # Convert the UUID to a string\n",
        "        }\n",
        "        images.append(image_info)\n",
        "        label_file = os.path.join(label_dir, os.path.basename(image_file).replace('.png', '.txt'))\n",
        "        with open(label_file, 'r') as file:\n",
        "            for line in file:\n",
        "                class_id, x_center, y_center, width, height = [float(x) for x in line.split()]\n",
        "                # Convert from YOLO to absolute COCO format\n",
        "                x_min = (x_center - width / 2) * 640\n",
        "                y_min = (y_center - height / 2) * 640\n",
        "                x_max = (x_center + width / 2) * 640\n",
        "                y_max = (y_center + height / 2) * 640\n",
        "                # width_abs = width * 640\n",
        "                # height_abs = height * 640\n",
        "                annotation = {\n",
        "                    'id': annotation_id,\n",
        "                    'image_id': str(image_id),  # Convert the UUID to a string\n",
        "                    'category_id': int(class_id),\n",
        "                    # 'bbox': [x_min, y_min, width_abs, height_abs],\n",
        "                    'bbox': [x_min, y_min, x_max - x_min, y_max - y_min],#\n",
        "                    'bbox_mode': BoxMode.XYXY_ABS,\n",
        "                    # 'area': width_abs * height_abs,\n",
        "                    'iscrowd': 0\n",
        "                }\n",
        "                annotations.append(annotation)\n",
        "                annotation_id += 1\n",
        "    coco_format = {\n",
        "        'images': images,\n",
        "        'annotations': annotations,\n",
        "        'categories': categories\n",
        "    }\n",
        "    with open(output_path, 'w') as output_file:\n",
        "        json.dump(coco_format, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTphQP-gv48C",
        "outputId": "5768ded2-1b6d-45bc-ec04-bf06afc05b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 7/7 [00:02<00:00,  2.65it/s]\n"
          ]
        }
      ],
      "source": [
        "yolo_to_coco(\"/content/drive/MyDrive/MyDrive/skin lesions/images/train\",'/content/drive/MyDrive/MyDrive/skin lesions/labels/train',  \"/content/drive/MyDrive/MyDrive/skin lesions/labels/train/annotations_train.json\",\"train\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBN4o5kPv7oB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from detectron2.structures import BoxMode\n",
        "def yolo_to_coco(image_dir, label_dir, output_path, dataset_type):\n",
        "    images = []\n",
        "    annotations = []\n",
        "    # Define category mappings\n",
        "    categories = [{'id': 0, 'name': 'unlabeled'}]  # Replace with your actual category names\n",
        "    annotation_id = 1\n",
        "    # Get all image files\n",
        "    image_files = glob(os.path.join(image_dir, '*.png'))\n",
        "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
        "        # Generate a random UUID for the image ID\n",
        "        image_id = uuid.uuid4()\n",
        "        image_info = {\n",
        "            'file_name': os.path.basename(image_file),\n",
        "            'height': 640,\n",
        "            'width': 640,\n",
        "            'id': str(image_id)  # Convert the UUID to a string\n",
        "        }\n",
        "        images.append(image_info)\n",
        "        label_file = os.path.join(label_dir, os.path.basename(image_file).replace('.png', '.txt'))\n",
        "        with open(label_file, 'r') as file:\n",
        "            for line in file:\n",
        "                class_id, x_center, y_center, width, height = [float(x) for x in line.split()]\n",
        "                # Convert from YOLO to absolute COCO format\n",
        "                x_min = (x_center - width / 2) * 640\n",
        "                y_min = (y_center - height / 2) * 640\n",
        "                x_max = (x_center + width / 2) * 640\n",
        "                y_max = (y_center + height / 2) * 640\n",
        "                # width_abs = width * 640\n",
        "                # height_abs = height * 640\n",
        "                annotation = {\n",
        "                    'id': annotation_id,\n",
        "                    'image_id': str(image_id),  # Convert the UUID to a string\n",
        "                    'category_id': int(class_id),\n",
        "                    # 'bbox': [x_min, y_min, width_abs, height_abs],\n",
        "                    'bbox': [x_min, y_min, x_max - x_min, y_max - y_min],#\n",
        "                    'bbox_mode': BoxMode.XYXY_ABS,\n",
        "                    # 'area': width_abs * height_abs,\n",
        "                    'iscrowd': 0\n",
        "                }\n",
        "                annotations.append(annotation)\n",
        "                annotation_id += 1\n",
        "    coco_format = {\n",
        "        'images': images,\n",
        "        'annotations': annotations,\n",
        "        'categories': categories\n",
        "    }\n",
        "    with open(output_path, 'w') as output_file:\n",
        "        json.dump(coco_format, output_file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fm5kaEav_dA",
        "outputId": "164983aa-64b1-4a6f-e248-bc706e3fa9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 5/5 [00:01<00:00,  2.74it/s]\n"
          ]
        }
      ],
      "source": [
        "yolo_to_coco(\"/content/drive/MyDrive/MyDrive/skin lesions/images/val\",'/content/drive/MyDrive/MyDrive/skin lesions/labels/val',  \"/content/drive/MyDrive/MyDrive/skin lesions/labels/train/annotations_val.json\",\"val\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgyRgIGl3xkW"
      },
      "outputs": [],
      "source": [
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader, DatasetCatalog, MetadataCatalog\n",
        "import torch  # Import torch for checking CUDA availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJO7ZEnlwCad",
        "outputId": "50c1f0b4-5aa3-40f3-96a9-b3b900c9b7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registered datasets:\n",
            "- coco_2014_train\n",
            "- coco_2014_val\n",
            "- coco_2014_minival\n",
            "- coco_2014_valminusminival\n",
            "- coco_2017_train\n",
            "- coco_2017_val\n",
            "- coco_2017_test\n",
            "- coco_2017_test-dev\n",
            "- coco_2017_val_100\n",
            "- keypoints_coco_2014_train\n",
            "- keypoints_coco_2014_val\n",
            "- keypoints_coco_2014_minival\n",
            "- keypoints_coco_2014_valminusminival\n",
            "- keypoints_coco_2017_train\n",
            "- keypoints_coco_2017_val\n",
            "- keypoints_coco_2017_val_100\n",
            "- coco_2017_train_panoptic_separated\n",
            "- coco_2017_train_panoptic_stuffonly\n",
            "- coco_2017_train_panoptic\n",
            "- coco_2017_val_panoptic_separated\n",
            "- coco_2017_val_panoptic_stuffonly\n",
            "- coco_2017_val_panoptic\n",
            "- coco_2017_val_100_panoptic_separated\n",
            "- coco_2017_val_100_panoptic_stuffonly\n",
            "- coco_2017_val_100_panoptic\n",
            "- lvis_v1_train\n",
            "- lvis_v1_val\n",
            "- lvis_v1_test_dev\n",
            "- lvis_v1_test_challenge\n",
            "- lvis_v0.5_train\n",
            "- lvis_v0.5_val\n",
            "- lvis_v0.5_val_rand_100\n",
            "- lvis_v0.5_test\n",
            "- lvis_v0.5_train_cocofied\n",
            "- lvis_v0.5_val_cocofied\n",
            "- cityscapes_fine_instance_seg_train\n",
            "- cityscapes_fine_sem_seg_train\n",
            "- cityscapes_fine_instance_seg_val\n",
            "- cityscapes_fine_sem_seg_val\n",
            "- cityscapes_fine_instance_seg_test\n",
            "- cityscapes_fine_sem_seg_test\n",
            "- cityscapes_fine_panoptic_train\n",
            "- cityscapes_fine_panoptic_val\n",
            "- voc_2007_trainval\n",
            "- voc_2007_train\n",
            "- voc_2007_val\n",
            "- voc_2007_test\n",
            "- voc_2012_trainval\n",
            "- voc_2012_train\n",
            "- voc_2012_val\n",
            "- ade20k_sem_seg_train\n",
            "- ade20k_sem_seg_val\n",
            "- coco_semi_1perc\n",
            "- coco_semi_2perc\n",
            "- coco_semi_5perc\n",
            "- coco_semi_10perc\n",
            "- coco_semi_20perc\n",
            "- coco_semi_30perc\n",
            "- coco_semi_40perc\n",
            "- coco_semi_50perc\n",
            "- coco_semi_60perc\n",
            "- coco_semi_80perc\n",
            "- cls_agnostic_coco\n",
            "- cls_agnostic_coco20k\n",
            "- imagenet_train\n",
            "- imagenet_train_r1\n",
            "- imagenet_train_r2\n",
            "- imagenet_train_r3\n",
            "- cls_agnostic_uvo\n",
            "- cls_agnostic_voc\n",
            "- cls_agnostic_clipart\n",
            "- cls_agnostic_watercolor\n",
            "- cls_agnostic_comic\n",
            "- cls_agnostic_kitti\n",
            "- cls_agnostic_openimages\n",
            "- cls_agnostic_objects365\n",
            "- cls_agnostic_lvis\n",
            "- train\n",
            "- val\n"
          ]
        }
      ],
      "source": [
        "register_dataset(\"train\", \"/content/drive/MyDrive/MyDrive/skin lesions/labels/train/annotations_train.json\", \"/content/drive/MyDrive/MyDrive/skin lesions/images/train\")\n",
        "register_dataset(\"val\", \"/content/drive/MyDrive/MyDrive/skin lesions/labels/val/annotations_val.json\", \"/content/drive/MyDrive/MyDrive/skin lesions/images/val\")\n",
        "list_registered_datasets()  # Call to list registered datasets for debugging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.modeling.roi_heads import ROI_HEADS_REGISTRY"
      ],
      "metadata": {
        "id": "XVwHO-D_EWXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFGA3x_SxTBd"
      },
      "outputs": [],
      "source": [
        "from detectron2.modeling.roi_heads import ROIHeads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from detectron2.modeling.roi_heads import ROI_HEADS_REGISTRY\n",
        "#from CutLER.cutler.modeling.roi_heads.custom_cascade_rcnn.py import CustomCascadeROIHeads\n",
        "\n",
        "# Register the custom ROI heads with a unique name\n",
        "#ROI_HEADS_REGISTRY.register(\"CustomCascadeROIHeads\", lambda: CustomCascadeROIHeads)"
      ],
      "metadata": {
        "id": "QxYBLSXHzvk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import inspect\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from detectron2.config import configurable\n",
        "from detectron2.layers import ShapeSpec, nonzero_tuple\n",
        "from detectron2.structures import Boxes, pairwise_iou\n",
        "from structures import pairwise_iou_max_scores\n",
        "from detectron2.structures import ImageList, Instances\n",
        "from detectron2.utils.events import get_event_storage\n",
        "from detectron2.utils.registry import Registry\n",
        "\n",
        "from detectron2.modeling.backbone.resnet import BottleneckBlock, ResNet\n",
        "from detectron2.modeling.matcher import Matcher\n",
        "from detectron2.modeling.poolers import ROIPooler\n",
        "from detectron2.modeling.proposal_generator.proposal_utils import add_ground_truth_to_proposals\n",
        "from detectron2.modeling.sampling import subsample_labels\n",
        "from detectron2.modeling.roi_heads.box_head import build_box_head\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
        "from detectron2.modeling.roi_heads.keypoint_head import build_keypoint_head\n",
        "from detectron2.modeling.roi_heads.mask_head import build_mask_head\n",
        "\n",
        "from detectron2.modeling.box_regression import Box2BoxTransform\n",
        "import torch.nn.functional as F\n",
        "from colored import fg\n",
        "\n",
        "\n",
        "if \"CustomStandardROIHeads\" not in ROI_HEADS_REGISTRY:\n",
        "    @ROI_HEADS_REGISTRY.register()\n",
        "    class CustomStandardROIHeads(ROIHeads):\n",
        "        \"\"\"\n",
        "        It's \"standard\" in a sense that there is no ROI transform sharing\n",
        "        or feature sharing between tasks.\n",
        "        Each head independently processes the input features by each head's\n",
        "        own pooler and head.\n",
        "\n",
        "        This class is used by most models, such as FPN and C5.\n",
        "        To implement more models, you can subclass it and implement a different\n",
        "        :meth:`forward()` or a head.\n",
        "        \"\"\"\n",
        "\n",
        "        @configurable\n",
        "        def __init__(\n",
        "            self,\n",
        "            *,\n",
        "            box_in_features: List[str],\n",
        "            box_pooler: ROIPooler,\n",
        "            box_head: nn.Module,\n",
        "            box_predictor: nn.Module,\n",
        "            mask_in_features: Optional[List[str]] = None,\n",
        "            mask_pooler: Optional[ROIPooler] = None,\n",
        "            mask_head: Optional[nn.Module] = None,\n",
        "            keypoint_in_features: Optional[List[str]] = None,\n",
        "            keypoint_pooler: Optional[ROIPooler] = None,\n",
        "            keypoint_head: Optional[nn.Module] = None,\n",
        "            train_on_pred_boxes: bool = False,\n",
        "            box2box_transform = Box2BoxTransform,\n",
        "            use_droploss: bool = False,\n",
        "            droploss_iou_thresh: float = 1.0,\n",
        "            **kwargs,\n",
        "        ):\n",
        "            \"\"\"\n",
        "            NOTE: this interface is experimental.\n",
        "\n",
        "            Args:\n",
        "                box_in_features (list[str]): list of feature names to use for the box head.\n",
        "                box_pooler (ROIPooler): pooler to extra region features for box head\n",
        "                box_head (nn.Module): transform features to make box predictions\n",
        "                box_predictor (nn.Module): make box predictions from the feature.\n",
        "                    Should have the same interface as :class:`FastRCNNOutputLayers`.\n",
        "                mask_in_features (list[str]): list of feature names to use for the mask\n",
        "                    pooler or mask head. None if not using mask head.\n",
        "                mask_pooler (ROIPooler): pooler to extract region features from image features.\n",
        "                    The mask head will then take region features to make predictions.\n",
        "                    If None, the mask head will directly take the dict of image features\n",
        "                    defined by `mask_in_features`\n",
        "                mask_head (nn.Module): transform features to make mask predictions\n",
        "                keypoint_in_features, keypoint_pooler, keypoint_head: similar to ``mask_*``.\n",
        "                train_on_pred_boxes (bool): whether to use proposal boxes or\n",
        "                    predicted boxes from the box head to train other heads.\n",
        "            \"\"\"\n",
        "            super().__init__(**kwargs)\n",
        "            # keep self.in_features for backward compatibility\n",
        "            self.in_features = self.box_in_features = box_in_features\n",
        "            self.box_pooler = box_pooler\n",
        "            self.box_head = box_head\n",
        "            self.box_predictor = box_predictor\n",
        "\n",
        "            self.mask_on = mask_in_features is not None\n",
        "            if self.mask_on:\n",
        "                self.mask_in_features = mask_in_features\n",
        "                self.mask_pooler = mask_pooler\n",
        "                self.mask_head = mask_head\n",
        "\n",
        "            self.keypoint_on = keypoint_in_features is not None\n",
        "            if self.keypoint_on:\n",
        "                self.keypoint_in_features = keypoint_in_features\n",
        "                self.keypoint_pooler = keypoint_pooler\n",
        "                self.keypoint_head = keypoint_head\n",
        "\n",
        "            self.train_on_pred_boxes = train_on_pred_boxes\n",
        "            self.use_droploss = use_droploss\n",
        "            self.box2box_transform = box2box_transform\n",
        "            self.droploss_iou_thresh = droploss_iou_thresh\n",
        "\n",
        "        @classmethod\n",
        "        def from_config(cls, cfg, input_shape):\n",
        "            ret = super().from_config(cfg)\n",
        "            ret[\"train_on_pred_boxes\"] = cfg.MODEL.ROI_BOX_HEAD.TRAIN_ON_PRED_BOXES\n",
        "            # Subclasses that have not been updated to use from_config style construction\n",
        "            # may have overridden _init_*_head methods. In this case, those overridden methods\n",
        "            # will not be classmethods and we need to avoid trying to call them here.\n",
        "            # We test for this with ismethod which only returns True for bound methods of cls.\n",
        "            # Such subclasses will need to handle calling their overridden _init_*_head methods.\n",
        "            if cfg.MODEL.ROI_HEADS.USE_DROPLOSS:\n",
        "                ret['use_droploss'] = True\n",
        "                ret['droploss_iou_thresh'] = cfg.MODEL.ROI_HEADS.DROPLOSS_IOU_THRESH\n",
        "                ret['box2box_transform'] = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
        "            if inspect.ismethod(cls._init_box_head):\n",
        "                ret.update(cls._init_box_head(cfg, input_shape))\n",
        "            if inspect.ismethod(cls._init_mask_head):\n",
        "                ret.update(cls._init_mask_head(cfg, input_shape))\n",
        "            if inspect.ismethod(cls._init_keypoint_head):\n",
        "                ret.update(cls._init_keypoint_head(cfg, input_shape))\n",
        "            return ret\n",
        "\n",
        "        @classmethod\n",
        "        def _init_box_head(cls, cfg, input_shape):\n",
        "            # fmt: off\n",
        "            in_features       = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
        "            pooler_resolution = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n",
        "            pooler_scales     = tuple(1.0 / input_shape[k].stride for k in in_features)\n",
        "            sampling_ratio    = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n",
        "            pooler_type       = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE\n",
        "            # fmt: on\n",
        "\n",
        "            # If CustomStandardROIHeads is applied on multiple feature maps (as in FPN),\n",
        "            # then we share the same predictors and therefore the channel counts must be the same\n",
        "            in_channels = [input_shape[f].channels for f in in_features]\n",
        "            # Check all channel counts are equal\n",
        "            assert len(set(in_channels)) == 1, in_channels\n",
        "            in_channels = in_channels[0]\n",
        "\n",
        "            box_pooler = ROIPooler(\n",
        "                output_size=pooler_resolution,\n",
        "                scales=pooler_scales,\n",
        "                sampling_ratio=sampling_ratio,\n",
        "                pooler_type=pooler_type,\n",
        "            )\n",
        "            # Here we split \"box head\" and \"box predictor\", which is mainly due to historical reasons.\n",
        "            # They are used together so the \"box predictor\" layers should be part of the \"box head\".\n",
        "            # New subclasses of ROIHeads do not need \"box predictor\"s.\n",
        "            box_head = build_box_head(\n",
        "                cfg, ShapeSpec(channels=in_channels, height=pooler_resolution, width=pooler_resolution)\n",
        "            )\n",
        "            box_predictor = FastRCNNOutputLayers(cfg, box_head.output_shape)\n",
        "            return {\n",
        "                \"box_in_features\": in_features,\n",
        "                \"box_pooler\": box_pooler,\n",
        "                \"box_head\": box_head,\n",
        "                \"box_predictor\": box_predictor,\n",
        "            }\n",
        "\n",
        "        @classmethod\n",
        "        def _init_mask_head(cls, cfg, input_shape):\n",
        "            if not cfg.MODEL.MASK_ON:\n",
        "                return {}\n",
        "            # fmt: off\n",
        "            in_features       = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
        "            pooler_resolution = cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\n",
        "            pooler_scales     = tuple(1.0 / input_shape[k].stride for k in in_features)\n",
        "            sampling_ratio    = cfg.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO\n",
        "            pooler_type       = cfg.MODEL.ROI_MASK_HEAD.POOLER_TYPE\n",
        "            # fmt: on\n",
        "\n",
        "            in_channels = [input_shape[f].channels for f in in_features][0]\n",
        "\n",
        "            ret = {\"mask_in_features\": in_features}\n",
        "            ret[\"mask_pooler\"] = (\n",
        "                ROIPooler(\n",
        "                    output_size=pooler_resolution,\n",
        "                    scales=pooler_scales,\n",
        "                    sampling_ratio=sampling_ratio,\n",
        "                    pooler_type=pooler_type,\n",
        "                )\n",
        "                if pooler_type\n",
        "                else None\n",
        "            )\n",
        "            if pooler_type:\n",
        "                shape = ShapeSpec(\n",
        "                    channels=in_channels, width=pooler_resolution, height=pooler_resolution\n",
        "                )\n",
        "            else:\n",
        "                shape = {f: input_shape[f] for f in in_features}\n",
        "            ret[\"mask_head\"] = build_mask_head(cfg, shape)\n",
        "            return ret\n",
        "\n",
        "        @classmethod\n",
        "        def _init_keypoint_head(cls, cfg, input_shape):\n",
        "            if not cfg.MODEL.KEYPOINT_ON:\n",
        "                return {}\n",
        "            # fmt: off\n",
        "            in_features       = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
        "            pooler_resolution = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_RESOLUTION\n",
        "            pooler_scales     = tuple(1.0 / input_shape[k].stride for k in in_features)  # noqa\n",
        "            sampling_ratio    = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_SAMPLING_RATIO\n",
        "            pooler_type       = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_TYPE\n",
        "            # fmt: on\n",
        "\n",
        "            in_channels = [input_shape[f].channels for f in in_features][0]\n",
        "\n",
        "            ret = {\"keypoint_in_features\": in_features}\n",
        "            ret[\"keypoint_pooler\"] = (\n",
        "                ROIPooler(\n",
        "                    output_size=pooler_resolution,\n",
        "                    scales=pooler_scales,\n",
        "                    sampling_ratio=sampling_ratio,\n",
        "                    pooler_type=pooler_type,\n",
        "                )\n",
        "                if pooler_type\n",
        "                else None\n",
        "            )\n",
        "            if pooler_type:\n",
        "                shape = ShapeSpec(\n",
        "                    channels=in_channels, width=pooler_resolution, height=pooler_resolution\n",
        "                )\n",
        "            else:\n",
        "                shape = {f: input_shape[f] for f in in_features}\n",
        "            ret[\"keypoint_head\"] = build_keypoint_head(cfg, shape)\n",
        "            return ret\n",
        "\n",
        "        def forward(\n",
        "            self,\n",
        "            images: ImageList,\n",
        "            features: Dict[str, torch.Tensor],\n",
        "            proposals: List[Instances],\n",
        "            targets: Optional[List[Instances]] = None,\n",
        "        ) -> Tuple[List[Instances], Dict[str, torch.Tensor]]:\n",
        "            \"\"\"\n",
        "            See :class:`ROIHeads.forward`.\n",
        "            \"\"\"\n",
        "            del images\n",
        "            if self.training:\n",
        "                assert targets, \"'targets' argument is required during training\"\n",
        "                proposals = self.label_and_sample_proposals(proposals, targets)\n",
        "            del targets\n",
        "\n",
        "            if self.training:\n",
        "                losses = self._forward_box(features, proposals)\n",
        "                # Usually the original proposals used by the box head are used by the mask, keypoint\n",
        "                # heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes\n",
        "                # predicted by the box head.\n",
        "                losses.update(self._forward_mask(features, proposals))\n",
        "                losses.update(self._forward_keypoint(features, proposals))\n",
        "                return proposals, losses\n",
        "            else:\n",
        "                pred_instances = self._forward_box(features, proposals)\n",
        "                # During inference cascaded prediction is used: the mask and keypoints heads are only\n",
        "                # applied to the top scoring box detections.\n",
        "                pred_instances = self.forward_with_given_boxes(features, pred_instances)\n",
        "                return pred_instances, {}\n",
        "\n",
        "        def forward_with_given_boxes(\n",
        "            self, features: Dict[str, torch.Tensor], instances: List[Instances]\n",
        "        ) -> List[Instances]:\n",
        "            \"\"\"\n",
        "            Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.\n",
        "\n",
        "            This is useful for downstream tasks where a box is known, but need to obtain\n",
        "            other attributes (outputs of other heads).\n",
        "            Test-time augmentation also uses this.\n",
        "\n",
        "            Args:\n",
        "                features: same as in `forward()`\n",
        "                instances (list[Instances]): instances to predict other outputs. Expect the keys\n",
        "                    \"pred_boxes\" and \"pred_classes\" to exist.\n",
        "\n",
        "            Returns:\n",
        "                list[Instances]:\n",
        "                    the same `Instances` objects, with extra\n",
        "                    fields such as `pred_masks` or `pred_keypoints`.\n",
        "            \"\"\"\n",
        "            assert not self.training\n",
        "            assert instances[0].has(\"pred_boxes\") and instances[0].has(\"pred_classes\")\n",
        "\n",
        "            instances = self._forward_mask(features, instances)\n",
        "            instances = self._forward_keypoint(features, instances)\n",
        "            return instances\n",
        "\n",
        "        def _forward_box(self, features: Dict[str, torch.Tensor], proposals: List[Instances]):\n",
        "            \"\"\"\n",
        "            Forward logic of the box prediction branch. If `self.train_on_pred_boxes is True`,\n",
        "                the function puts predicted boxes in the `proposal_boxes` field of `proposals` argument.\n",
        "\n",
        "            Args:\n",
        "                features (dict[str, Tensor]): mapping from feature map names to tensor.\n",
        "                    Same as in :meth:`ROIHeads.forward`.\n",
        "                proposals (list[Instances]): the per-image object proposals with\n",
        "                    their matching ground truth.\n",
        "                    Each has fields \"proposal_boxes\", and \"objectness_logits\",\n",
        "                    \"gt_classes\", \"gt_boxes\".\n",
        "\n",
        "            Returns:\n",
        "                In training, a dict of losses.\n",
        "                In inference, a list of `Instances`, the predicted instances.\n",
        "            \"\"\"\n",
        "            features = [features[f] for f in self.box_in_features]\n",
        "            box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals]) # torch.Size([512 * batch_size, 256, 7, 7])\n",
        "            box_features = self.box_head(box_features) # torch.Size([512 * batch_size, 1024])\n",
        "            predictions = self.box_predictor(box_features) # [torch.Size([512 * batch_size, 2]), torch.Size([512 * batch_size, 4])]\n",
        "\n",
        "            no_gt_found = False\n",
        "            if self.use_droploss and self.training:\n",
        "                # the first K proposals are GT proposals\n",
        "                try:\n",
        "                    box_num_list = [len(x.gt_boxes) for x in proposals]\n",
        "                    gt_num_list = [torch.unique(x.gt_boxes.tensor[:100], dim=0).size()[0] for x in proposals]\n",
        "                except:\n",
        "                    box_num_list = [0 for _ in proposals]\n",
        "                    gt_num_list = [0 for _ in proposals]\n",
        "                    no_gt_found = True\n",
        "\n",
        "            if self.use_droploss and self.training and not no_gt_found:\n",
        "                # NOTE: maximum overlapping with GT (IoU)\n",
        "                predictions_delta = predictions[1]\n",
        "                proposal_boxes = Boxes.cat([x.proposal_boxes for x in proposals])\n",
        "                predictions_bbox = self.box2box_transform.apply_deltas(predictions_delta, proposal_boxes.tensor)\n",
        "                idx_start = 0\n",
        "                iou_max_list = []\n",
        "                for idx, x in enumerate(proposals):\n",
        "                    idx_end = idx_start + box_num_list[idx]\n",
        "                    iou_max_list.append(pairwise_iou_max_scores(predictions_bbox[idx_start:idx_end], x.gt_boxes[:gt_num_list[idx]].tensor))\n",
        "                    idx_start = idx_end\n",
        "                iou_max = torch.cat(iou_max_list, dim=0)\n",
        "\n",
        "            del box_features\n",
        "\n",
        "            if self.training:\n",
        "                if self.use_droploss and not no_gt_found:\n",
        "                    weights = iou_max.le(self.droploss_iou_thresh).float()\n",
        "                    weights = 1 - weights.ge(1.0).float()\n",
        "                    losses = self.box_predictor.losses(predictions, proposals, weights=weights.detach())\n",
        "                else:\n",
        "                    losses = self.box_predictor.losses(predictions, proposals)\n",
        "                if self.train_on_pred_boxes: # default is false\n",
        "                    with torch.no_grad():\n",
        "                        pred_boxes = self.box_predictor.predict_boxes_for_gt_classes(\n",
        "                            predictions, proposals\n",
        "                        )\n",
        "                        for proposals_per_image, pred_boxes_per_image in zip(proposals, pred_boxes):\n",
        "                            proposals_per_image.proposal_boxes = Boxes(pred_boxes_per_image)\n",
        "                return losses\n",
        "            else:\n",
        "                pred_instances, _ = self.box_predictor.inference(predictions, proposals)\n",
        "                return pred_instances\n",
        "\n",
        "        def _forward_mask(self, features: Dict[str, torch.Tensor], instances: List[Instances]):\n",
        "            \"\"\"\n",
        "            Forward logic of the mask prediction branch.\n",
        "\n",
        "            Args:\n",
        "                features (dict[str, Tensor]): mapping from feature map names to tensor.\n",
        "                    Same as in :meth:`ROIHeads.forward`.\n",
        "                instances (list[Instances]): the per-image instances to train/predict masks.\n",
        "                    In training, they can be the proposals.\n",
        "                    In inference, they can be the boxes predicted by R-CNN box head.\n",
        "\n",
        "            Returns:\n",
        "                In training, a dict of losses.\n",
        "                In inference, update `instances` with new fields \"pred_masks\" and return it.\n",
        "            \"\"\"\n",
        "            if not self.mask_on:\n",
        "                return {} if self.training else instances\n",
        "\n",
        "            if self.training:\n",
        "                # head is only trained on positive proposals.\n",
        "                instances, _ = select_foreground_proposals(instances, self.num_classes)\n",
        "\n",
        "            if self.mask_pooler is not None:\n",
        "                features = [features[f] for f in self.mask_in_features]\n",
        "                boxes = [x.proposal_boxes if self.training else x.pred_boxes for x in instances]\n",
        "                features = self.mask_pooler(features, boxes)\n",
        "            else:\n",
        "                features = {f: features[f] for f in self.mask_in_features}\n",
        "            return self.mask_head(features, instances)\n",
        "\n",
        "        def _forward_keypoint(self, features: Dict[str, torch.Tensor], instances: List[Instances]):\n",
        "            \"\"\"\n",
        "            Forward logic of the keypoint prediction branch.\n",
        "\n",
        "            Args:\n",
        "                features (dict[str, Tensor]): mapping from feature map names to tensor.\n",
        "                    Same as in :meth:`ROIHeads.forward`.\n",
        "                instances (list[Instances]): the per-image instances to train/predict keypoints.\n",
        "                    In training, they can be the proposals.\n",
        "                    In inference, they can be the boxes predicted by R-CNN box head.\n",
        "\n",
        "            Returns:\n",
        "                In training, a dict of losses.\n",
        "                In inference, update `instances` with new fields \"pred_keypoints\" and return it.\n",
        "            \"\"\"\n",
        "            if not self.keypoint_on:\n",
        "                return {} if self.training else instances\n",
        "\n",
        "            if self.training:\n",
        "                # head is only trained on positive proposals with >=1 visible keypoints.\n",
        "                instances, _ = select_foreground_proposals(instances, self.num_classes)\n",
        "                instances = select_proposals_with_visible_keypoints(instances)\n",
        "\n",
        "            if self.keypoint_pooler is not None:\n",
        "                features = [features[f] for f in self.keypoint_in_features]\n",
        "                boxes = [x.proposal_boxes if self.training else x.pred_boxes for x in instances]\n",
        "                features = self.keypoint_pooler(features, boxes)\n",
        "            else:\n",
        "                features = {f: features[f] for f in self.keypoint_in_features}\n",
        "            return self.keypoint_head(features, instances)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tvz_-MHJ914K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# Modified by XuDong Wang from https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/roi_heads/cascade_rcnn.py\n",
        "\n",
        "from typing import List\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd.function import Function\n",
        "\n",
        "from detectron2.config import configurable\n",
        "from detectron2.layers import ShapeSpec\n",
        "from detectron2.structures import Boxes, pairwise_iou\n",
        "from structures import pairwise_iou_max_scores\n",
        "from detectron2.structures import Instances\n",
        "from detectron2.utils.events import get_event_storage\n",
        "\n",
        "from detectron2.modeling.box_regression import Box2BoxTransform\n",
        "from detectron2.modeling.matcher import Matcher\n",
        "from detectron2.modeling.poolers import ROIPooler\n",
        "from detectron2.modeling.roi_heads.box_head import build_box_head\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, fast_rcnn_inference\n",
        "from detectron2.modeling.roi_heads import ROI_HEADS_REGISTRY\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class _ScaleGradient(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, scale):\n",
        "        ctx.scale = scale\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output * ctx.scale, None\n",
        "\n",
        "if \"CustomCascadeROIHeads\"  not in ROI_HEADS_REGISTRY:\n",
        "    @ROI_HEADS_REGISTRY.register()\n",
        "    class CustomCascadeROIHeads(CustomStandardROIHeads):\n",
        "        \"\"\"\n",
        "        The ROI heads that implement :paper:`Cascade R-CNN`.\n",
        "        \"\"\"\n",
        "\n",
        "        @configurable\n",
        "        def __init__(\n",
        "            self,\n",
        "            *,\n",
        "            box_in_features: List[str],\n",
        "            box_pooler: ROIPooler,\n",
        "            box_heads: List[nn.Module],\n",
        "            box_predictors: List[nn.Module],\n",
        "            proposal_matchers: List[Matcher],\n",
        "            **kwargs,\n",
        "        ):\n",
        "            \"\"\"\n",
        "            NOTE: this interface is experimental.\n",
        "\n",
        "            Args:\n",
        "                box_pooler (ROIPooler): pooler that extracts region features from given boxes\n",
        "                box_heads (list[nn.Module]): box head for each cascade stage\n",
        "                box_predictors (list[nn.Module]): box predictor for each cascade stage\n",
        "                proposal_matchers (list[Matcher]): matcher with different IoU thresholds to\n",
        "                    match boxes with ground truth for each stage. The first matcher matches\n",
        "                    RPN proposals with ground truth, the other matchers use boxes predicted\n",
        "                    by the previous stage as proposals and match them with ground truth.\n",
        "            \"\"\"\n",
        "            assert \"proposal_matcher\" not in kwargs, (\n",
        "                \"CustomCascadeROIHeads takes 'proposal_matchers=' for each stage instead \"\n",
        "                \"of one 'proposal_matcher='.\"\n",
        "            )\n",
        "            # The first matcher matches RPN proposals with ground truth, done in the base class\n",
        "            kwargs[\"proposal_matcher\"] = proposal_matchers[0]\n",
        "            num_stages = self.num_cascade_stages = len(box_heads)\n",
        "            box_heads = nn.ModuleList(box_heads)\n",
        "            box_predictors = nn.ModuleList(box_predictors)\n",
        "            assert len(box_predictors) == num_stages, f\"{len(box_predictors)} != {num_stages}!\"\n",
        "            assert len(proposal_matchers) == num_stages, f\"{len(proposal_matchers)} != {num_stages}!\"\n",
        "            super().__init__(\n",
        "                box_in_features=box_in_features,\n",
        "                box_pooler=box_pooler,\n",
        "                box_head=box_heads,\n",
        "                box_predictor=box_predictors,\n",
        "                **kwargs,\n",
        "            )\n",
        "            self.proposal_matchers = proposal_matchers\n",
        "\n",
        "        @classmethod\n",
        "        def from_config(cls, cfg, input_shape):\n",
        "            ret = super().from_config(cfg, input_shape)\n",
        "            ret.pop(\"proposal_matcher\")\n",
        "            return ret\n",
        "\n",
        "        @classmethod\n",
        "        def _init_box_head(cls, cfg, input_shape):\n",
        "            # fmt: off\n",
        "            in_features              = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
        "            pooler_resolution        = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n",
        "            pooler_scales            = tuple(1.0 / input_shape[k].stride for k in in_features)\n",
        "            sampling_ratio           = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n",
        "            pooler_type              = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE\n",
        "            cascade_bbox_reg_weights = cfg.MODEL.ROI_BOX_CASCADE_HEAD.BBOX_REG_WEIGHTS\n",
        "            cascade_ious             = cfg.MODEL.ROI_BOX_CASCADE_HEAD.IOUS\n",
        "            assert len(cascade_bbox_reg_weights) == len(cascade_ious)\n",
        "            assert cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG,  \\\n",
        "                \"CustomCascadeROIHeads only support class-agnostic regression now!\"\n",
        "            assert cascade_ious[0] == cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS[0]\n",
        "            # fmt: on\n",
        "\n",
        "            in_channels = [input_shape[f].channels for f in in_features]\n",
        "            # Check all channel counts are equal\n",
        "            assert len(set(in_channels)) == 1, in_channels\n",
        "            in_channels = in_channels[0]\n",
        "\n",
        "            box_pooler = ROIPooler(\n",
        "                output_size=pooler_resolution,\n",
        "                scales=pooler_scales,\n",
        "                sampling_ratio=sampling_ratio,\n",
        "                pooler_type=pooler_type,\n",
        "            )\n",
        "            pooled_shape = ShapeSpec(\n",
        "                channels=in_channels, width=pooler_resolution, height=pooler_resolution\n",
        "            )\n",
        "\n",
        "            box_heads, box_predictors, proposal_matchers = [], [], []\n",
        "            for match_iou, bbox_reg_weights in zip(cascade_ious, cascade_bbox_reg_weights):\n",
        "                box_head = build_box_head(cfg, pooled_shape)\n",
        "                box_heads.append(box_head)\n",
        "                box_predictors.append(\n",
        "                    FastRCNNOutputLayers(\n",
        "                        cfg,\n",
        "                        box_head.output_shape,\n",
        "                        box2box_transform=Box2BoxTransform(weights=bbox_reg_weights),\n",
        "                    )\n",
        "                )\n",
        "                proposal_matchers.append(Matcher([match_iou], [0, 1], allow_low_quality_matches=False))\n",
        "            return {\n",
        "                \"box_in_features\": in_features,\n",
        "                \"box_pooler\": box_pooler,\n",
        "                \"box_heads\": box_heads,\n",
        "                \"box_predictors\": box_predictors,\n",
        "                \"proposal_matchers\": proposal_matchers,\n",
        "            }\n",
        "\n",
        "    def forward(self, images, features, proposals, targets=None):\n",
        "        del images\n",
        "        if self.training:\n",
        "            proposals = self.label_and_sample_proposals(proposals, targets)\n",
        "\n",
        "        if self.training:\n",
        "            # Need targets to box head\n",
        "            losses = self._forward_box(features, proposals, targets)\n",
        "            losses.update(self._forward_mask(features, proposals))\n",
        "            losses.update(self._forward_keypoint(features, proposals))\n",
        "            return proposals, losses\n",
        "        else:\n",
        "            pred_instances = self._forward_box(features, proposals)\n",
        "            pred_instances = self.forward_with_given_boxes(features, pred_instances)\n",
        "            return pred_instances, {}\n",
        "\n",
        "    def _forward_box(self, features, proposals, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features, targets: the same as in\n",
        "                Same as in :meth:`ROIHeads.forward`.\n",
        "            proposals (list[Instances]): the per-image object proposals with\n",
        "                their matching ground truth.\n",
        "                Each has fields \"proposal_boxes\", and \"objectness_logits\",\n",
        "                \"gt_classes\", \"gt_boxes\".\n",
        "        \"\"\"\n",
        "        features = [features[f] for f in self.box_in_features]\n",
        "        head_outputs = []  # (predictor, predictions, proposals)\n",
        "        prev_pred_boxes = None\n",
        "        image_sizes = [x.image_size for x in proposals]\n",
        "        for k in range(self.num_cascade_stages):\n",
        "            if k > 0:\n",
        "                # The output boxes of the previous stage are used to create the input\n",
        "                # proposals of the next stage.\n",
        "                proposals = self._create_proposals_from_boxes(prev_pred_boxes, image_sizes)\n",
        "                if self.training:\n",
        "                    proposals = self._match_and_label_boxes(proposals, k, targets)\n",
        "            predictions = self._run_stage(features, proposals, k)\n",
        "            prev_pred_boxes = self.box_predictor[k].predict_boxes(predictions, proposals)\n",
        "            head_outputs.append((self.box_predictor[k], predictions, proposals))\n",
        "\n",
        "        no_gt_found = False\n",
        "        if self.training:\n",
        "            losses = {}\n",
        "            storage = get_event_storage()\n",
        "            for stage, (predictor, predictions, proposals) in enumerate(head_outputs):\n",
        "                no_gt_found = False\n",
        "                with storage.name_scope(\"stage{}\".format(stage)):\n",
        "                    if self.use_droploss:\n",
        "                        try:\n",
        "                            box_num_list = [len(x.gt_boxes) for x in proposals]\n",
        "                            gt_num_list = [torch.unique(x.gt_boxes.tensor[:100], dim=0).size()[0] for x in proposals]\n",
        "                        except:\n",
        "                            box_num_list = [0 for x in proposals]\n",
        "                            gt_num_list = [0 for x in proposals]\n",
        "                            no_gt_found = True\n",
        "\n",
        "                        if not no_gt_found:\n",
        "                            # NOTE: confidence score\n",
        "                            prediction_score, predictions_delta = predictions[0], predictions[1]\n",
        "                            prediction_score = F.softmax(prediction_score, dim=1)[:,0]\n",
        "\n",
        "                            # NOTE: maximum overlapping with GT (IoU)\n",
        "                            proposal_boxes = Boxes.cat([x.proposal_boxes for x in proposals])\n",
        "                            predictions_bbox = predictor.box2box_transform.apply_deltas(predictions_delta, proposal_boxes.tensor)\n",
        "                            idx_start = 0\n",
        "                            iou_max_list = []\n",
        "                            for idx, x in enumerate(proposals):\n",
        "                                idx_end = idx_start + box_num_list[idx]\n",
        "                                iou_max_list.append(pairwise_iou_max_scores(predictions_bbox[idx_start:idx_end], x.gt_boxes[:gt_num_list[idx]].tensor))\n",
        "                                idx_start = idx_end\n",
        "                            iou_max = torch.cat(iou_max_list, dim=0)\n",
        "\n",
        "                            # NOTE: get the weight of each proposal\n",
        "                            weights = iou_max.le(self.droploss_iou_thresh).float()\n",
        "                            weights = 1 - weights.ge(1.0).float()\n",
        "                            stage_losses = predictor.losses(predictions, proposals, weights=weights.detach())\n",
        "                        else:\n",
        "                            stage_losses = predictor.losses(predictions, proposals)\n",
        "                    else:\n",
        "                        stage_losses = predictor.losses(predictions, proposals)\n",
        "                losses.update({k + \"_stage{}\".format(stage): v for k, v in stage_losses.items()})\n",
        "            return losses\n",
        "        else:\n",
        "            # Each is a list[Tensor] of length #image. Each tensor is Ri x (K+1)\n",
        "            scores_per_stage = [h[0].predict_probs(h[1], h[2]) for h in head_outputs]\n",
        "\n",
        "            # Average the scores across heads\n",
        "            scores = [\n",
        "                sum(list(scores_per_image)) * (1.0 / self.num_cascade_stages)\n",
        "                for scores_per_image in zip(*scores_per_stage)\n",
        "            ]\n",
        "            # Use the boxes of the last head\n",
        "            predictor, predictions, proposals = head_outputs[-1]\n",
        "            boxes = predictor.predict_boxes(predictions, proposals)\n",
        "            pred_instances, _ = fast_rcnn_inference(\n",
        "                boxes,\n",
        "                scores,\n",
        "                image_sizes,\n",
        "                predictor.test_score_thresh,\n",
        "                predictor.test_nms_thresh,\n",
        "                predictor.test_topk_per_image,\n",
        "            )\n",
        "            return pred_instances\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _match_and_label_boxes(self, proposals, stage, targets):\n",
        "        \"\"\"\n",
        "        Match proposals with groundtruth using the matcher at the given stage.\n",
        "        Label the proposals as foreground or background based on the match.\n",
        "\n",
        "        Args:\n",
        "            proposals (list[Instances]): One Instances for each image, with\n",
        "                the field \"proposal_boxes\".\n",
        "            stage (int): the current stage\n",
        "            targets (list[Instances]): the ground truth instances\n",
        "\n",
        "        Returns:\n",
        "            list[Instances]: the same proposals, but with fields \"gt_classes\" and \"gt_boxes\"\n",
        "        \"\"\"\n",
        "        num_fg_samples, num_bg_samples = [], []\n",
        "        for proposals_per_image, targets_per_image in zip(proposals, targets):\n",
        "            match_quality_matrix = pairwise_iou(\n",
        "                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes\n",
        "            )\n",
        "            # proposal_labels are 0 or 1\n",
        "            matched_idxs, proposal_labels = self.proposal_matchers[stage](match_quality_matrix)\n",
        "            if len(targets_per_image) > 0:\n",
        "                gt_classes = targets_per_image.gt_classes[matched_idxs]\n",
        "                # Label unmatched proposals (0 label from matcher) as background (label=num_classes)\n",
        "                gt_classes[proposal_labels == 0] = self.num_classes\n",
        "                gt_boxes = targets_per_image.gt_boxes[matched_idxs]\n",
        "            else:\n",
        "                gt_classes = torch.zeros_like(matched_idxs) + self.num_classes\n",
        "                gt_boxes = Boxes(\n",
        "                    targets_per_image.gt_boxes.tensor.new_zeros((len(proposals_per_image), 4))\n",
        "                )\n",
        "            proposals_per_image.gt_classes = gt_classes\n",
        "            proposals_per_image.gt_boxes = gt_boxes\n",
        "\n",
        "            num_fg_samples.append((proposal_labels == 1).sum().item())\n",
        "            num_bg_samples.append(proposal_labels.numel() - num_fg_samples[-1])\n",
        "\n",
        "        # Log the number of fg/bg samples in each stage\n",
        "        storage = get_event_storage()\n",
        "        storage.put_scalar(\n",
        "            \"stage{}/roi_head/num_fg_samples\".format(stage),\n",
        "            sum(num_fg_samples) / len(num_fg_samples),\n",
        "        )\n",
        "        storage.put_scalar(\n",
        "            \"stage{}/roi_head/num_bg_samples\".format(stage),\n",
        "            sum(num_bg_samples) / len(num_bg_samples),\n",
        "        )\n",
        "        return proposals\n",
        "\n",
        "    def _run_stage(self, features, proposals, stage):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features (list[Tensor]): #lvl input features to ROIHeads\n",
        "            proposals (list[Instances]): #image Instances, with the field \"proposal_boxes\"\n",
        "            stage (int): the current stage\n",
        "\n",
        "        Returns:\n",
        "            Same output as `FastRCNNOutputLayers.forward()`.\n",
        "        \"\"\"\n",
        "        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
        "        # The original implementation averages the losses among heads,\n",
        "        # but scale up the parameter gradients of the heads.\n",
        "        # This is equivalent to adding the losses among heads,\n",
        "        # but scale down the gradients on features.\n",
        "        if self.training:\n",
        "            box_features = _ScaleGradient.apply(box_features, 1.0 / self.num_cascade_stages)\n",
        "        box_features = self.box_head[stage](box_features)\n",
        "        return self.box_predictor[stage](box_features)\n",
        "\n",
        "    def _create_proposals_from_boxes(self, boxes, image_sizes):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            boxes (list[Tensor]): per-image predicted boxes, each of shape Ri x 4\n",
        "            image_sizes (list[tuple]): list of image shapes in (h, w)\n",
        "\n",
        "        Returns:\n",
        "            list[Instances]: per-image proposals with the given boxes.\n",
        "        \"\"\"\n",
        "        # Just like RPN, the proposals should not have gradients\n",
        "        boxes = [Boxes(b.detach()) for b in boxes]\n",
        "        proposals = []\n",
        "        for boxes_per_image, image_size in zip(boxes, image_sizes):\n",
        "            boxes_per_image.clip(image_size)\n",
        "            if self.training:\n",
        "                # do not filter empty boxes at inference time,\n",
        "                # because the scores from each stage need to be aligned and added later\n",
        "                boxes_per_image = boxes_per_image[boxes_per_image.nonempty()]\n",
        "            prop = Instances(image_size)\n",
        "            prop.proposal_boxes = boxes_per_image\n",
        "            proposals.append(prop)\n",
        "        return proposals\n"
      ],
      "metadata": {
        "id": "EYdsXE2L24jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayh_4BOD31d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce59f64d-3fc0-4b9e-d404-69f0489a8c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/02 09:18:50 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(\n",
            "      256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output2): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral3): Conv2d(\n",
            "      512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output3): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral4): Conv2d(\n",
            "      1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output4): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_lateral5): Conv2d(\n",
            "      2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fpn_output5): Conv2d(\n",
            "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "      (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): CustomCascadeROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): ModuleList(\n",
            "      (0-2): 3 x FastRCNNConvFCHead(\n",
            "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "        (fc_relu1): ReLU()\n",
            "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (fc_relu2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (box_predictor): ModuleList(\n",
            "      (0-2): 3 x FastRCNNOutputLayers(\n",
            "        (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [04/02 09:18:50 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[04/02 09:18:50 d2.data.datasets.coco]: Loaded 7 images in COCO format from /content/drive/MyDrive/MyDrive/skin lesions/labels/train/annotations_train.json\n",
            "[04/02 09:18:50 d2.data.build]: Removed 0 images with no usable annotations. 7 images left.\n",
            "[04/02 09:18:50 d2.data.build]: Distribution of instances among all 1 categories:\n",
            "|  category  | #instances   |\n",
            "|:----------:|:-------------|\n",
            "| unlabeled  | 17           |\n",
            "|            |              |\n",
            "[04/02 09:18:50 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640,), max_size=640, sample_style='choice'), RandomFlip()]\n",
            "[04/02 09:18:50 d2.data.build]: Using training sampler TrainingSampler\n",
            "[04/02 09:18:50 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/02 09:18:50 d2.data.common]: Serializing 7 elements to byte tensors and concatenating them all ...\n",
            "[04/02 09:18:50 d2.data.common]: Serialized dataset takes 0.00 MiB\n",
            "[04/02 09:18:50 d2.data.build]: Making batched data loader with batch_size=16\n",
            "WARNING [04/02 09:18:50 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[04/02 09:18:50 d2.data.datasets.coco]: Loaded 7 images in COCO format from /content/drive/MyDrive/MyDrive/skin lesions/labels/train/annotations_train.json\n",
            "[04/02 09:18:50 d2.data.build]: Removed 0 images with no usable annotations. 7 images left.\n",
            "[04/02 09:18:50 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640,), max_size=640, sample_style='choice'), RandomFlip()]\n",
            "[04/02 09:18:50 d2.data.build]: Using training sampler TrainingSampler\n",
            "[04/02 09:18:50 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/02 09:18:50 d2.data.common]: Serializing 7 elements to byte tensors and concatenating them all ...\n",
            "[04/02 09:18:50 d2.data.common]: Serialized dataset takes 0.00 MiB\n",
            "[04/02 09:18:50 d2.data.build]: Making batched data loader with batch_size=16\n",
            "[04/02 09:18:50 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_cascade_final.pth ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "cutler_cascade_final.pth: 575MB [00:02, 256MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/02 09:18:53 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR [04/02 09:19:03 d2.engine.train_loop]: Exception during training:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 155, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 496, in run_step\n",
            "    self._trainer.run_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 310, in run_step\n",
            "    loss_dict = self.model(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/modeling/meta_arch/rcnn.py\", line 167, in forward\n",
            "    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-30-17efa0737d13>\", line 252, in forward\n",
            "    losses = self._forward_box(features, proposals)\n",
            "  File \"<ipython-input-30-17efa0737d13>\", line 312, in _forward_box\n",
            "    box_features = self.box_head(box_features) # torch.Size([512 * batch_size, 1024])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 374, in _forward_unimplemented\n",
            "    raise NotImplementedError(f\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\")\n",
            "NotImplementedError: Module [ModuleList] is missing the required \"forward\" function\n",
            "[04/02 09:19:03 d2.engine.hooks]: Total training time: 0:00:09 (0:00:00 on hooks)\n",
            "[04/02 09:19:03 d2.utils.events]:  iter: 0       lr: N/A  max_mem: 14338M\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Module [ModuleList] is missing the required \"forward\" function",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-d5e01030d087>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \"\"\"\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             assert hasattr(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# self.iter == max_iter can be used by `after_train` to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdo\u001b[0m \u001b[0msomething\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myou\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_period\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_event_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-17efa0737d13>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;31m# Usually the original proposals used by the box head are used by the mask, keypoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;31m# heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-17efa0737d13>\u001b[0m in \u001b[0;36m_forward_box\u001b[0;34m(self, features, proposals)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_in_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_pooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_boxes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([512 * batch_size, 256, 7, 7])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([512 * batch_size, 1024])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [torch.Size([512 * batch_size, 2]), torch.Size([512 * batch_size, 4])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
          ]
        }
      ],
      "source": [
        "\n",
        "cfg = get_cfg()\n",
        "add_cutler_config(cfg)\n",
        "cfg.merge_from_file('/content/CutLER/cutler/model_zoo/configs/COCO-Semisupervised/cascade_mask_rcnn_R_50_FPN_100perc.yaml')\n",
        "cfg.DATASETS.TRAIN = (\"train\",)\n",
        "cfg.DATASETS.TEST = (\"val\",)\n",
        "cfg.MODEL.ROI_HEADS.NAME =\"CustomCascadeROIHeads\"\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.WEIGHTS = \"http://dl.fbaipublicfiles.com/cutler/checkpoints/cutler_cascade_final.pth\"\n",
        "cfg.INPUT.MIN_SIZE_TRAIN = (640, )\n",
        "cfg.INPUT.MAX_SIZE_TRAIN = 640\n",
        "cfg.INPUT.MIN_SIZE_TEST = 640\n",
        "cfg.INPUT.MAX_SIZE_TEST = 640\n",
        "cfg.SOLVER.IMS_PER_BATCH = 16\n",
        "cfg.SOLVER.BASE_LR = 0.0025\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.SOLVER.STEPS = [25, 50]\n",
        "cfg.SOLVER.WARMUP_ITERS = 10\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "cfg.SEED = 10000\n",
        "cfg.OUTPUT_DIR = './train1'\n",
        "cfg.OUTPUT_ENABLED = True\n",
        "cfg.MODEL.DEVICE == \"cpu\" #and cfg.MODEL.RESNETS.NORM == 'SyncBN':\n",
        "#cfg.MODEL.RESNETS.NORM = \"BN\"\n",
        "#cfg.MODEL.FPN.NORM = \"BN\"\n",
        "# (Optional) Modify loss function if needed (refer to Detectron2 documentation)\n",
        "\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.modeling.roi_heads import ROI_HEADS_REGISTRY\n",
        "from CutLER.cutler.modeling.roi_heads.custom_cascade_rcnn.py import CustomCascadeROIHeads\n",
        "\n",
        "# Register the custom ROI heads with a unique name\n",
        "ROI_HEADS_REGISTRY.register(\"custom_cascade_roi_heads\", lambda: CustomCascadeROIHeads)\n"
      ],
      "metadata": {
        "id": "02lru8ALISjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4cngh-FPKaGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROI_HEADS_REGISTRY.register(\"CustomCascadeROIHeads\")"
      ],
      "metadata": {
        "id": "K1xRAxrRMv7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p0ifkZT_POpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}